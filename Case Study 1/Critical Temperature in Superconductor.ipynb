{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a13135-9c5d-415c-bed4-49f46e078e69",
   "metadata": {},
   "source": [
    "<center>Joaquin Dominguez, Richard Kim, Hien Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da441ed2-a94e-40fc-b96e-e5130019575e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Paragraph here on background of superconductor and why critical temperature is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f21667-6612-4a67-a3a5-cc59b1a96b4e",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Data Preprocessing\n",
    "The superconductivity dataset consisted of two files: train and material. The former file contained 82 relevant feature information from 21,263 superconductors including the response feature, `critical_temp`. The material file contained 88 features which represented the chemical formula for `material` that were one hot encoded for all 21,263 superconductors. A simplistic example for water (H2O) would have a two under the hydrogen column, one under oxygen, and zero for all the other elements. The two files were concatenated and `material` subsequently dropped since this feature denoted the full chemical formula and deemed unnecessary after hot encoding. Next, we identified the nine features that had a single value and consequently dropped them due to their lack of usefulness in predictive performance. Lastly, we confirmed that the data was free of missing values and duplicated records. The final data frame resulted in 21263 rows and 159 columns. We will note the datatypes were all numerical and proceeded with exploratory data analysis.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "Mention/discuss regression assumptions?\n",
    "\n",
    "## Feature Scaling\n",
    "Given that the features followed a normal distribution, we felt confident Scikit-Learn’s standard scaler class was the most appropriate feature scaling for the dataset. This estimator bounded each feature to maintain a mean of zero and standard deviation of one. We conducted linear regression with scaled and unscaled data and compared their performance by way of five-fold internal cross validation. Looking at R2 and root mean squared error (RMSE), both models produced equivalent metrics at 0.45 and 23.8, respectively. We split the data into X and y (training features and target feature, respectively) and proceeded with standard scaler going forward.\n",
    "\n",
    "## Modeling\n",
    "As stated above, we would like to predict the critical temperature of a superconductor using linear regression with regularization then investigate the important features. Specifically, lasso and ridge regularization were explored. Briefly explain what lasso and ridge is here.\n",
    "We utilized Scikit-Learn’s <i>pipeline</i> class to set up the gridsearch workflow, lasso and ridge classes to train the models, gridsearchcv class to execute the search and refit the best performing model and cross_validate class to ascertain their mean RMSE from five-fold, shuffled internal cross validation. We tuned lasso’s alpha to be between 0.1-10 with 200 samples and ridge’s alpha to be 0.1-100 with 300 samples. The feature importance was extracted from the best model of each algorithm and its values transformed to the absolute root because we care about the magnitude of the coefficients, not the sign of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78023517-e8b3-45c9-a9ca-19ab8b23c68c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results\n",
    "\n",
    "## Regularization\n",
    "Displayed below is a table of the optimal alpha from each model and their respective five-fold cross validation metrics.  \n",
    "\n",
    "|   | Lasso | Ridge |\n",
    "| :- | -: | :-: |\n",
    "| Alpha | 2.03 | 100\n",
    "| R2 | 0.66 | 0.53\n",
    "| RMSE | 20.09 | 22.43\n",
    "\n",
    "## Feature Importance\n",
    "Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8656f-00d8-441a-802f-1316ec03eb01",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Linear regression with lasso regularization produced the lowest RMSE, albeit quite small difference, but also 13% R2 increase with fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de724c82-5144-42fb-b8e6-05957b5f4e57",
   "metadata": {},
   "source": [
    "# Code\n",
    "Please refer to the attached Python notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
