{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a13135-9c5d-415c-bed4-49f46e078e69",
   "metadata": {},
   "source": [
    "<center>Joaquin Dominguez, Richard Kim, Hien Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da441ed2-a94e-40fc-b96e-e5130019575e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Paragraph here on background of superconductor and why critical temperature is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f21667-6612-4a67-a3a5-cc59b1a96b4e",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Data Preprocessing\n",
    "The superconductivity dataset consisted of two files: train and material. The former file contained 82 relevant feature information from 21,263 superconductors including the response feature, `critical_temp`. The material file contained 88 features which represented the chemical formula for `material` that were one hot encoded for all 21,263 superconductors. A simplistic example for water ($H_{2}O$) would have a two under the hydrogen column, one under oxygen, and zero for all the other elements. The two files were concatenated and `material` was subsequently dropped, since this feature denoted the full chemical formula and was deemed unnecessary after hot encoding. Next, we identified the nine features that had a single value and consequently dropped them due to their lack of usefulness in predictive performance. Lastly, we confirmed that the data was free of missing values and duplicated records. The final data frame resulted in 21263 rows and 159 columns. We will note the datatypes were all numerical and proceeded with exploratory data analysis.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "Mention/discuss regression assumptions?\n",
    "\n",
    "## Feature Scaling\n",
    "Given that the features followed a normal distribution, we felt confident Scikit-Learn’s standard scaler class was the most appropriate feature scaling for the dataset. This estimator bounded each feature to maintain a mean of zero and standard deviation of one. We conducted linear regression with scaled and unscaled data and compared their performance by way of five-fold internal cross validation. Looking at $R^{2}$ and root mean squared error (RMSE), both models produced equivalent metrics at 0.45 and 23.8, respectively. We split the data into X and y (training features and target feature, respectively) and proceeded with standard scaler going forward.\n",
    "\n",
    "## Modeling\n",
    "As stated above, we would like to predict the critical temperature of a superconductor using linear regression with regularization, then investigate the important features. Specifically, lasso and ridge regularization were explored. Lasso regularization (L1) is commonly used for variable selection because it induces sparsity with the coefficients. As the value of lambda increases and consequently increases the penalty, coefficients of variables with weaker contributions are forced to 0. Variables that are allowed to retain a non-zero value can then be identified as important variables. In contrast, Ridge Regularization (L2) is more commonly used to prevent overfitting. L2 is different from L1 regularization in that it does not induce sparsity but rather applies the penalty more uniformly. The result is the full list of coefficients, where all of the variables are inhibited in their growth and none of the variables are forced to 0. \n",
    "\n",
    "Given our goal to investigate feature importance, an L1 model does make more conceptual sense. Nonetheless, we proceed by creating both models. We utilized Scikit-Learn’s <i>pipeline</i> class to set up the gridsearch workflow, <i>lasso</i> and <i>ridge</i> classes to train the models, <i>gridsearchcv</i> class to execute the hyperparameter search then refit the best performing model and <i>cross_validate</i> class to ascertain their mean RMSE from five-fold, shuffled internal cross validation. The feature importance was extracted from the best model of each algorithm and its values transformed to the absolute root because we care about the magnitude of the coefficients, not the sign of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78023517-e8b3-45c9-a9ca-19ab8b23c68c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results\n",
    "\n",
    "## Regularization\n",
    "We conducted a lasso and ridge gridsearch with hyperparameter tuning. Lasso’s alpha was set to be between 0.1-10 with 200 samples and ridge’s alpha to be 0.1-100 with 300 samples. Displayed below is a table of the optimal alpha from each model and their respective five-fold cross validation metrics.  \n",
    "\n",
    "|   | Lasso | Ridge |\n",
    "| :- | -: | :-: |\n",
    "| Alpha | 2.03 | 100\n",
    "| $R^{2}$ | 0.66 | 0.53\n",
    "| RMSE | 20.09 | 22.43\n",
    "\n",
    "## Feature Importance\n",
    "Given that the data were standardized prior to modeling, the coefficients across our variables are on the same scale. This allows for direct comparisons, where we may interpret larger coefficients as indicative of greater contribution or importance. Although the positive and negative signs of the coefficients provide information on directionality, we focus on the magnitude of the coefficients and summarize our findings on feature importance below. \n",
    "\n",
    "The most important features identified by our L1 model were, in order, `wtd_std_ThermalConductivity`, `Ba`,`wtd_entropy_atomic_mass`,`Ca`, and `wtd_gmean_ElectronAffinity`. By contrast, our L2 model identified `wtd_std_ThermalConductivity`, `std_ElectronAffinity`, `range_ElectronAffinity`, `Ba`, and `range_atomic_mass\t` as the most important variables. \n",
    "\n",
    "We found that `wtd_std_ThermalConductivity` came up with the greatest contribution in both cases, and `Ba` fell into the top 5 important variables in both of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8656f-00d8-441a-802f-1316ec03eb01",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The linear regression model using lasso regularization produced a lower RMSE at 20.09, as well as a larger $R^{2}$ value at 0.66. By comparison, the model using ridge regularization had an RMSE of 22.43 and an $R^{2}$ value of 0.53. These results indicate that our lasso model is a better fit than the ridge model. This, in addition to the fact that lasso regularization is generally more applicable for identifying important features, lends support to its preferred use over the ridge model for this case study. \n",
    "\n",
    "In line with these results, we highlight our findings from the lasso model, concluding that the most important feature in predicting critical temperatures is `wtd_std_ThermalConductivity`. `Ba` was identified as the second most important feature, which was also the only other feature that the ridge model shared in the top 5 list. Above all other features, these two variables have the highest predictive power for critical temperatures after accounting for possible overfitting. It may also be worth noting that measures surrounding electron affinity consistently showed up as relatively important features in both of our models. Despite our findings here, this additional insight may warrant a deeper look at the relation between electron affinity and critical temperature.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de724c82-5144-42fb-b8e6-05957b5f4e57",
   "metadata": {},
   "source": [
    "# Code\n",
    "Please refer to the attached Python notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
