{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Case Study 3:</center></h1>\n",
    "<h2><center>Spam Classifier, Bayes and Clustering</center></h2>\n",
    "<h3>Authors:</h3>\n",
    "Joaquin Dominguez <br>\n",
    "Richard Kim <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing/Merging from local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The path must contain all folders with emails\n",
    "- Folders must have 'ham' or 'spam' in its name, and the rest of the directory must not include 'ham'\n",
    "- If 'ham' is included in a spam folder's directory, spam emails may labeled 'ham' later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2\n",
      "easy_ham_2\n",
      "easy_ham\n",
      "hard_ham\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders_path = '/home/joaquindominguez/Documents/QTW/case_studies/QTW_CaseStudy/Case Study 3/Data'\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(folders_path, topdown=False):\n",
    "    for name in files:\n",
    "        tmp = os.path.join(root,name)\n",
    "        file_list.append(tmp)\n",
    "    for item in dirs:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9353"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of files\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Emails: Text vs Multipart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Email messages are parsed into 1 of 2 arrays, text emails and multipart emails\n",
    "- Content types (e.g. text) and labels (e.g. spam) are saved in separate arrays for each category (text vs multipart)\n",
    "- Directories are saved as unique IDs\n",
    "- Messages with multipart content types that **DID NOT contain content types for each of its parts** were treated like single texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-One\n",
    "import email\n",
    "\n",
    "text_list = []\n",
    "\n",
    "messages_list = []\n",
    "type_list = []\n",
    "labels = []\n",
    "\n",
    "uniq_mult_list = []\n",
    "mult_list = []\n",
    "\n",
    "mult_messages_list = []\n",
    "mult_type_list = []\n",
    "mult_labels = []\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    with open(file_list[i],'r',encoding='latin1') as f:        \n",
    "        message = email.message_from_file(f)\n",
    "        body = message.get_payload()\n",
    "        content_type = message.get_content_type()\n",
    "        \n",
    "        if 'text' in content_type: \n",
    "            text_list.append(file_list[i])\n",
    "            messages_list.append(body)\n",
    "            if 'ham' in file_list[i]:\n",
    "                labels.append(0)\n",
    "            else: \n",
    "                labels.append(1)\n",
    "            type_list.append(content_type)\n",
    "        elif 'mult' in content_type: \n",
    "            uniq_mult_list.append(file_list[i])\n",
    "            if 'text' in body:\n",
    "                mult_list.append(file_list[i])\n",
    "                mult_messages_list.append(body)\n",
    "                mult_type_list.append(content_type)\n",
    "                if 'ham' in file_list[i]:\n",
    "                    mult_labels.append(0)\n",
    "                else: \n",
    "                    mult_labels.append(1)\n",
    "            else: \n",
    "                for j in body: \n",
    "                    if 'text' in j.get_content_type(): \n",
    "                        mult_list.append(file_list[i])\n",
    "                        mult_messages_list.append(j.get_payload())\n",
    "                        mult_type_list.append(j.get_content_type())\n",
    "                        if 'ham' in file_list[i]:\n",
    "                            mult_labels.append(0)\n",
    "                        else: \n",
    "                            mult_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Total Number of Files**\n",
      "9353 \n",
      "\n",
      "**Text Emails**\n",
      "Text Email Count: 8607\n",
      "Messages: 8607\n",
      "Spam/Ham Labels: 8607\n",
      "Content Type Labels: 8607 \n",
      "\n",
      "**Multipart Emails**\n",
      "Multipart Email Count: 746\n",
      "Separated Messages: 1034\n",
      "Spam/Ham Labels: 1034\n",
      "Content Type Labels: 1034 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All-in-one count of files\n",
    "print('**Total Number of Files**')\n",
    "print(len(file_list), '\\n')\n",
    "\n",
    "print('**Text Emails**')\n",
    "print('Text Email Count:',len(text_list))\n",
    "print('Messages:',len(messages_list))\n",
    "print('Spam/Ham Labels:',len(labels))\n",
    "print('Content Type Labels:',len(type_list), '\\n')\n",
    "\n",
    "print('**Multipart Emails**')\n",
    "print('Multipart Email Count:',len(uniq_mult_list))\n",
    "print('Separated Messages:',len(mult_messages_list))\n",
    "print('Spam/Ham Labels:',len(mult_labels))\n",
    "print('Content Type Labels:',len(mult_type_list), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_df = pd.DataFrame({'directory':text_list,'message':messages_list,'spam1':labels,'content type':type_list})\n",
    "mult_df = pd.DataFrame({'directory':mult_list,'message':mult_messages_list,'spam1':mult_labels,'content type':mult_type_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many directories come up more than once (indicating 2+ messages from a multipart email)\n",
    "dups = mult_df.groupby(mult_df['directory'],as_index=False).size()\n",
    "multipart_df = pd.merge(mult_df, dups, on='directory', how='left')\n",
    "test = multipart_df[multipart_df['size'] > 1]\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#print(test['message'])\n",
    "sum(multipart_df['size'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory       /home/joaquindominguez/Documents/QTW/case_stud...\n",
      "message         <html>\\n<head>\\n<title>Free Sizzling LTC Sales...\n",
      "spam1                                                           1\n",
      "content type                                            text/html\n",
      "size                                                            2\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')\n",
    "print(multipart_df.iloc[1].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.rebackee.com/cursos2/contraloria.htm\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = ''\n",
    "text_list = []\n",
    "for i in range(len(multipart_df)):\n",
    "    val = multipart_df.iloc[i,1]\n",
    "    soup = BeautifulSoup(val,'lxml')\n",
    "    text = soup.get_text().lower()\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    stop_words = stopwords.words('english')\n",
    "    words_list = [w for w in text.split() if w not in stop_words]\n",
    "    words_list = [lemmatizer.lemmatize(w) for w in words_list]\n",
    "    words_list = [stemmer.stem(w) for w in words_list]\n",
    "    text_list.append(' '.join(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipart_df['proc_text'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.post-gazette.com/columnists/20020905brian5.asp\n",
      "\n",
      "\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "text_list = []\n",
    "for i in range(len(text_df)):\n",
    "    val = text_df.iloc[i,1]\n",
    "    soup = BeautifulSoup(val,'lxml')\n",
    "    text = soup.get_text().lower()\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    stop_words = stopwords.words('english')\n",
    "    words_list = [w for w in text.split() if w not in stop_words]\n",
    "    words_list = [lemmatizer.lemmatize(w) for w in words_list]\n",
    "    words_list = [stemmer.stem(w) for w in words_list]\n",
    "    text_list.append(' '.join(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['proc_text'] = text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix (additional codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Emails (multipart directories saved elsewhere)\n",
    "import email\n",
    "text_list = []\n",
    "messages_list = []\n",
    "type_list = []\n",
    "labels = []\n",
    "\n",
    "mult_list = []\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    with open(file_list[i],'r',encoding='latin1') as f:     \n",
    "        text_list.append(file_list[i])   \n",
    "        message = email.message_from_file(f)\n",
    "        body = message.get_payload()\n",
    "        content_type = message.get_content_type()\n",
    "        \n",
    "        if 'text' in content_type: \n",
    "            messages_list.append(body)\n",
    "            if 'ham' in file_list[i]:\n",
    "                labels.append(0)\n",
    "            else: \n",
    "                labels.append(1)\n",
    "            type_list.append(content_type)\n",
    "        elif 'mult' in content_type: \n",
    "            mult_list.append(file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multipart (keep parts where content types are text)\n",
    "mult_messages_list = []\n",
    "mult_type_list = []\n",
    "mult_labels = []\n",
    "\n",
    "for i in range(len(mult_list)):\n",
    "    with open(mult_list[i],'r',encoding='latin1') as f:\n",
    "        messages = email.message_from_file(f)\n",
    "        body = messages.get_payload()\n",
    "        content_type = messages.get_content_type()\n",
    "\n",
    "        if 'text' in body:\n",
    "            mult_messages_list.append(body)\n",
    "            mult_type_list.append(content_type)\n",
    "            if 'ham' in mult_list[i]:\n",
    "                mult_labels.append(0)\n",
    "            else: \n",
    "                mult_labels.append(1)\n",
    "        else: \n",
    "            for j in body: \n",
    "                if 'text' in j.get_content_type(): \n",
    "                    mult_messages_list.append(j.get_payload())\n",
    "                    mult_type_list.append(j.get_content_type())\n",
    "                    if 'ham' in mult_list[i]:\n",
    "                        mult_labels.append(0)\n",
    "                    else: \n",
    "                        mult_labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Individual Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HABERDAR.COM - HABER VE MEDYA PORTALI\n",
      "Artýk tüm haberleri sadece tek siteden takip edebileceksiniz. Haberdar.com açýldý!\n",
      "Haber baþlýklarý, spor haberleri, teknoloji haberleri, kültür ve sanat haberleri, internet haberleri, bilim ve uzay, \n",
      "sinema, saðlýk...\n",
      "Aradýðýnýz içerik http://www.haberdar.com adresinde\n",
      "Sadece týklayýn ve haberdar olun\n",
      "\n",
      "ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÓ+,ùÞµéX¬²'²Þu¼ÿ9 Íý8«yÚ\u001f¶\u0016­±©¢W\\zYiÞüg­jw°êÞ~ÅDAÿÛi³ÿÿÃ\fÿza¢xýÊ&þ¿Ú²ë­Ç¢¸\u001e×úÞ}Ê\u001d{³}ýÓÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿJ+^±Ê(¥êÿµ©d¨¥x%ËR×¬r)z¿íjYÿ+-³û(º·\u001e~à{ùÞ¶\u001bm¦ÏÿþX¬¶Ïì¢êÜyú+ïçzßåËlþX¬¶)ß£û\"µë\u001c¢^¯ûZ\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Codeblock for looking at individual emails\n",
    "filename = 'spam/00116.29e39a0064e2714681726ac28ff3fdef'\n",
    "\n",
    "import os\n",
    "with open(os.path.join('/home/joaquindominguez/Documents/QTW/case_studies/QTW_CaseStudy/Case Study 3/Data/',filename),'r',encoding='latin1') as f: \n",
    "    message = email.message_from_file(f)\n",
    "    body = message.get_payload()\n",
    "    print(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
